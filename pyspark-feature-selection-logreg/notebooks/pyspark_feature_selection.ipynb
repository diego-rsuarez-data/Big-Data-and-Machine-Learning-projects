{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Second Assignment: Machine Learning with Spark**\n"
      ],
      "metadata": {
        "id": "U4sMvd-Nsg71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authors: Alfonso Delgado Lara & Diego Rivera Suárez"
      ],
      "metadata": {
        "id": "p9dg5lRkQDX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary objective of this project is to develop a scalable classification model to predict the success of telemarketing campaigns for a banking institution. Specifically, the goal is to predict whether a client will subscribe to a term deposit (binary classification: \"yes\" or \"no\"). This problem is critical for optimizing marketing strategies, reducing resource wastage, and increasing conversation rates.\n",
        "\n",
        "To handle the data processing and modeling efficiently, we utilized Apache Spark (PySpark), leveraging its MLlib library to create robust Machine Learning pipelines. Furthermore, the core classification algorithm selected for this study is Logistic Regression, chosen for its interpretability and efficiency in binary classification tasks. Finally, this assignment will focus heavily on the use of the Universal Feature Selection and its importance in model performance and intrpretability, highlighting its important as a tool for reducing overfitting and computational inefficiency."
      ],
      "metadata": {
        "id": "CGkdGLKLQNtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading the data"
      ],
      "metadata": {
        "id": "kY2Rs1A1sr28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step for completing this study will be to upload the dataset, which is the same one used for the first assignment."
      ],
      "metadata": {
        "id": "YNVAswdSSX-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TI3mCc26sX0k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_pickle(\"bank_92.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first assignment, we were asked to preprocess and modify the **pdays** variable, since it is not usable still for classification purposes. Its preprocessing has been replicated from the previous assignment and has not been included in a pipeline, but rather has been applied now, so that the dataset has an optimal structure, before being transformed to a .csv format. As for the preprocessing itself:\n",
        "\n",
        "The pdays variable records the number of days since the client was last contacted, with -1 indicating that the client was never contacted. Machine learning models can misinterpret such placeholder values, so pdays is transformed into two new features:\n",
        "\n",
        "- **contacted_before**, is a binary indicator where 1 means the client was previously contacted (pdays > -1) and 0 means never contacted (pdays = -1).\n",
        "\n",
        "- **days_since_last_contact**, preserves the original pdays value for previously contacted clients, while assigning max(pdays) + 100 to clients never contacted.\n",
        "\n",
        "This transformation allows both tree-based and distance-based models to interpret the information effectively, treating \"never contacted\" as a meaningful and distinct situation."
      ],
      "metadata": {
        "id": "FZk5SsN-0hVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing pdays using a custom transformer\n",
        "#This transformer creates two new features and drops the original pdays column\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class PdaysTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.max_pdays = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        #Find the maximum pdays for clients who were previously contacted\n",
        "        self.max_pdays = X.loc[X['pdays'] > -1, 'pdays'].max()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        #Create binary feature: 1 if client was contacted before, 0 otherwise\n",
        "        X['contacted_before'] = (X['pdays'] > -1).astype(int)\n",
        "\n",
        "        #Create numeric feature for days since last contact\n",
        "        #Assign max_pdays + 100 to clients never contacted\n",
        "        X['days_since_last_contact'] = X['pdays'].apply(\n",
        "            lambda x: x if x > -1 else self.max_pdays + 100\n",
        "        )\n",
        "\n",
        "        # Drop the original pdays column\n",
        "        X = X.drop(columns=['pdays'])\n",
        "        return X\n",
        "\n",
        "transformer = PdaysTransformer()\n",
        "df = transformer.fit_transform(df)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1QvZ9MH0oqh",
        "outputId": "309a37e8-5d14-44e0-ddc2-eb9e5276c65f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age         job  marital  education default  balance housing loan  contact  \\\n",
            "0   59      admin.  married  secondary      no     2343     yes   no  unknown   \n",
            "1   56      admin.  married  secondary      no       45      no   no  unknown   \n",
            "2   41  technician  married  secondary      no     1270     yes   no  unknown   \n",
            "3   55    services  married  secondary      no     2476     yes   no  unknown   \n",
            "4   54      admin.  married   tertiary      no      184      no   no  unknown   \n",
            "\n",
            "   day month  duration  campaign  previous poutcome deposit  contacted_before  \\\n",
            "0    5   may      1042         1         0  unknown     yes                 0   \n",
            "1    5   may      1467         1         0  unknown     yes                 0   \n",
            "2    5   may      1389         1         0  unknown     yes                 0   \n",
            "3    5   may       579         1         0  unknown     yes                 0   \n",
            "4    5   may       673         2         0  unknown     yes                 0   \n",
            "\n",
            "   days_since_last_contact  \n",
            "0                      954  \n",
            "1                      954  \n",
            "2                      954  \n",
            "3                      954  \n",
            "4                      954  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the treatment of the **pdays** variable has come to an end, the dataset can already be converted into a csv file."
      ],
      "metadata": {
        "id": "pU2NP_i63nxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('bank_92.csv', index=False)"
      ],
      "metadata": {
        "id": "T7U2nZMC3sZ8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Spark Session"
      ],
      "metadata": {
        "id": "_ANXr9SPuDjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark is the Python API for Apache Spark, a powerful open-source distributed computing framework. It allows developers to harness the simplicity of the Python language while leveraging Spark’s ability to process massive datasets in parallel across a cluster of computers. It effectively bridges the gap between data science in Python and big data engineering, enabling scalable data processing and Machine Learning (via the MLlib library).\n",
        "\n",
        "This snippet initializes the Spark environment, establishing the necessary connection to the computing cluster. It creates a SparkSession, which serves as the unified entry point for programming with DataFrames, and exposes the SparkContext (sc) to coordinate task execution across the distributed worker nodes."
      ],
      "metadata": {
        "id": "otrZSd-EVyr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Second Assignment: Machine Learning with Spark\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(spark)\n",
        "print(sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhrvXGSyuHiR",
        "outputId": "ab872575-4ae4-45df-980a-efc9d2b5d9eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7baa98ccd3d0>\n",
            "<SparkContext master=local[*] appName=Second Assignment: Machine Learning with Spark>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, we upload our dataset to our Spark Session:"
      ],
      "metadata": {
        "id": "BCELSmxhvlGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ava_sd=spark.read.csv(path='bank_92.csv',header=True,inferSchema=True)\n",
        "ava_sd.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTGnyYOgv4mV",
        "outputId": "05fb0c1c-241c-48ba-f2ff-c2d846a6d14e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+--------+--------+-------+----------------+-----------------------+\n",
            "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|previous|poutcome|deposit|contacted_before|days_since_last_contact|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+--------+--------+-------+----------------+-----------------------+\n",
            "| 59|     admin.| married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 56|     admin.| married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 41| technician| married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 55|   services| married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 54|     admin.| married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|       0| unknown|    yes|               0|                    954|\n",
            "| 42| management|  single| tertiary|     no|      0|    yes| yes|unknown|  5|  may|     562|       2|       0| unknown|    yes|               0|                    954|\n",
            "| 56| management| married| tertiary|     no|    830|    yes| yes|unknown|  6|  may|    1201|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 60|    retired|divorced|secondary|     no|    545|    yes|  no|unknown|  6|  may|    1030|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 37| technician| married|secondary|     no|      1|    yes|  no|unknown|  6|  may|     608|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 28|   services|  single|secondary|     no|   5090|    yes|  no|unknown|  6|  may|    1297|       3|       0| unknown|    yes|               0|                    954|\n",
            "| 38|     admin.|  single|secondary|     no|    100|    yes|  no|unknown|  7|  may|     786|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 30|blue-collar| married|secondary|     no|    309|    yes|  no|unknown|  7|  may|    1574|       2|       0| unknown|    yes|               0|                    954|\n",
            "| 29| management| married| tertiary|     no|    199|    yes| yes|unknown|  7|  may|    1689|       4|       0| unknown|    yes|               0|                    954|\n",
            "| 46|blue-collar|  single| tertiary|     no|    460|    yes|  no|unknown|  7|  may|    1102|       2|       0| unknown|    yes|               0|                    954|\n",
            "| 31| technician|  single| tertiary|     no|    703|    yes|  no|unknown|  8|  may|     943|       2|       0| unknown|    yes|               0|                    954|\n",
            "| 35| management|divorced| tertiary|     no|   3837|    yes|  no|unknown|  8|  may|    1084|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 32|blue-collar|  single|  primary|     no|    611|    yes|  no|unknown|  8|  may|     541|       3|       0| unknown|    yes|               0|                    954|\n",
            "| 49|   services| married|secondary|     no|     -8|    yes|  no|unknown|  8|  may|    1119|       1|       0| unknown|    yes|               0|                    954|\n",
            "| 41|     admin.| married|secondary|     no|     55|    yes|  no|unknown|  8|  may|    1120|       2|       0| unknown|    yes|               0|                    954|\n",
            "| 49|     admin.|divorced|secondary|     no|    168|    yes| yes|unknown|  8|  may|     513|       1|       0| unknown|    yes|               0|                    954|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+--------+--------+-------+----------------+-----------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are going to prepare the data so that they can be used for our PySpark Machine Learning Environment:"
      ],
      "metadata": {
        "id": "vzj9nlRPxcRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sd = ava_sd.withColumnRenamed(\"deposit\", \"label\")\n",
        "\n",
        "ignore = ['label']\n"
      ],
      "metadata": {
        "id": "8QP53bm8xrTi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing Strategy\n",
        "\n",
        "**Why is this step necessary?**\n",
        "\n",
        "Machine Learning algorithms, such as Logistic Regression, rely on mathematical equations that require numerical input. Our raw dataset, however, contains **categorical variables** (strings like \"married\" or \"admin\"). Furthermore, PySpark's **MLlib** library has a specific architectural requirement: it expects all input features to be consolidated into a **single vector column**. Therefore, this preprocessing stage is essential to transform human-readable text into a machine-readable numerical format without introducing statistical bias.\n",
        "\n",
        "The code implements a transformation pipeline consisting of three key stages:\n",
        "\n",
        "1. **Label Indexing:**\n",
        "   The target variable **label** is currently a string (\"yes\"/\"no\"). We use **StringIndexer** to convert this into a numerical label (0.0 or 1.0), allowing the algorithm to calculate the classification error.\n",
        "\n",
        "2. **Categorical Encoding (The Loop):**\n",
        "   We iterate through each categorical variable **cat_cols** and apply a two-step transformation:\n",
        "   * **StringIndexer:** First, maps each unique string category to a numerical index based on frequency.\n",
        "   * **OneHotEncoder:** Converts these indices into binary vectors. This step is vital for nominal variables to prevent the model from assuming an artificial ordinal relationship (e.g., incorrectly assuming that job category 2 is \"greater than\" job category 1).\n",
        "\n",
        "3. **Feature Assembly:**\n",
        "   Finally, the **VectorAssembler** takes the newly created One-Hot Encoded vectors and combines them with the original numerical columns **num_cols**. This results in a single column named **features**, containing a comprehensive vector representation of each client.\n",
        "\n",
        "All these steps are wrapped in a **Pipeline**, ensuring that the sequence of transformations is applied consistently to both training and validation data."
      ],
      "metadata": {
        "id": "o_Q4Hx_Zzl0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cat_cols = ['job', 'marital', 'education', 'default', 'housing',\n",
        "            'loan', 'contact', 'poutcome','month']\n",
        "num_cols = ['age','balance','duration','campaign','contacted_before',\n",
        "            'days_since_last_contact','previous','day']\n",
        "\n",
        "# Now, we are going to create the Pipeline for preprocessing the categorical\n",
        "# features, since we cannot use the VectorAssembler function if we have them.\n",
        "\n",
        "stages = []\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
        "stages += [label_indexer]\n",
        "\n",
        "for categoricalCol in cat_cols:\n",
        "    # First, we use stringIndexer for tranforming categories to a number\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol,\n",
        "                                  outputCol=categoricalCol + \"_index\",\n",
        "                                  handleInvalid=\"keep\")\n",
        "\n",
        "    # Then, we use OneHotEncoder for transforming the indices into a OHE vector\n",
        "    #  (ej: 0 -> [1,0], 1 -> [0,1])\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
        "                            outputCols=[categoricalCol + \"_vec\"],\n",
        "                            handleInvalid=\"keep\")\n",
        "\n",
        "    stages += [stringIndexer, encoder]\n",
        "\n",
        "\n",
        "# Now, we join the OHE new columns with the original numerical ones:\n",
        "\n",
        "assemblerInputs = [c + \"_vec\" for c in cat_cols] + num_cols\n",
        "\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]\n",
        "\n",
        "# And the final Pipeline for this preprocessing part will be:\n",
        "\n",
        "pipeline1= Pipeline(stages=stages)"
      ],
      "metadata": {
        "id": "23hlQbzZzs81"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have constructed it, now we are able to transform the dataset to the appropiate ML language in PySpark, fitting our data to the previously constructed Pipeline."
      ],
      "metadata": {
        "id": "AB7TTZwc63x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_model = pipeline1.fit(data_sd)\n",
        "dataFinal = pipeline_model.transform(data_sd)\n",
        "\n",
        "dataFinal = dataFinal \\\n",
        "    .drop(\"label\") \\\n",
        "    .withColumnRenamed(\"label_index\", \"label\")\n",
        "\n",
        "dataFinal.select('label', 'features').show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMj3LDi-7G0i",
        "outputId": "a37d15d9-3c46-4e00-bd77-d2fe05d22f5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------------------------------------------------------------------------------------------------+\n",
            "|label|features                                                                                                                 |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------+\n",
            "|1.0  |(61,[3,13,17,22,26,28,32,35,40,53,54,55,56,58,60],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,59.0,2343.0,1042.0,1.0,954.0,5.0])|\n",
            "|1.0  |(61,[3,13,17,22,25,28,32,35,40,53,54,55,56,58,60],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,56.0,45.0,1467.0,1.0,954.0,5.0])  |\n",
            "|1.0  |(61,[2,13,17,22,26,28,32,35,40,53,54,55,56,58,60],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,41.0,1270.0,1389.0,1.0,954.0,5.0])|\n",
            "|1.0  |(61,[4,13,17,22,26,28,32,35,40,53,54,55,56,58,60],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,55.0,2476.0,579.0,1.0,954.0,5.0]) |\n",
            "|1.0  |(61,[3,13,18,22,25,28,32,35,40,53,54,55,56,58,60],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,54.0,184.0,673.0,2.0,954.0,5.0])  |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the model, we must split the dataset to simulate how the model will perform on unseen data. Given our dataset size of approximately **11,000 records**, we selected an **80/20 split ratio**:\n",
        "\n",
        "* **Training Set (80% ~ 8,800 rows):** This provides the algorithm with sufficient data to learn the underlying patterns and relationships between features without underfitting.\n",
        "* **Validation Set (20% ~ 2,200 rows):** This size is statistically significant enough to evaluate the model's performance reliably. Since marketing datasets are often **imbalanced** (fewer \"yes\" responses), a smaller validation set (e.g., 10%) might result in too few positive examples to calculate a stable AUC or Accuracy. The 20% hold-out ensures the evaluation metrics are robust."
      ],
      "metadata": {
        "id": "bil9898789WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 100566492\n",
        "\n",
        "train, validation = dataFinal.randomSplit([0.8, 0.2], seed=random_seed)"
      ],
      "metadata": {
        "id": "nuLBbOrB888u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE SELECTION APPROACHES"
      ],
      "metadata": {
        "id": "rmGrNzlt-nbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the preprocessing stage, specifically the One-Hot Encoding of categorical variables, the dimensionality of our dataset has increased significantly (from the original input columns to over 60 generated features). While this provides granular information, a high number of features can lead to **overfitting**, increased computational cost, and \"noise\" that confuses the model.\n",
        "\n",
        "To address this, we employ the **UnivariateFeatureSelector**. This tool evaluates each feature individually against the target variable **label** using statistical tests (ANOVA F-test) to determine the strength of the relationship.\n",
        "\n",
        "\n",
        "\n",
        "We will conduct a comparative analysis using **four distinct strategies** to identify the optimal balance between model complexity and performance:\n",
        "\n",
        "1.  **Baseline (No Selection):**\n",
        "    We utilize all generated features. This serves as the control group to measure if feature selection actually improves or degrades the model.\n",
        "\n",
        "2.  **FPR (False Positive Rate):**\n",
        "    A **least conservative** strategy. It selects features based on a flexible p-value threshold, allowing more features to pass through. It prioritizes keeping potential signals, even at the risk of including some noise.\n",
        "\n",
        "3.  **FWE (Family-Wise Error Rate):**\n",
        "    A **most conservative** strategy. It applies a strict penalty to the p-values to control the probability of making even a single false discovery. This results in a very lean model containing only the most statistically significant features.\n",
        "\n",
        "4.  **Percentile (Top 25%):**\n",
        "    A pragmatic approach that ignores p-value thresholds and simply selects the top 25% of features with the highest statistical scores. This forces a fixed \"budget\" of features (reducing the 61 inputs to approximately 15).\n",
        "\n",
        "*Note: For the UnivariateFeatureSelector to function correctly with our mixed dataset in PySpark, we configure the selector to treat all features as \"continuous,\" allowing the underlying ANOVA test to assess both numerical and binary (dummy) variables uniformly.*"
      ],
      "metadata": {
        "id": "RK0GoHhy_40f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To predict the binary outcome **label**, we employ **Logistic Regression**. Despite its name, this is a linear model used for classification rather than regression. It estimates the probability of an event occurring (e.g., the probability of a client saying \"yes\") by mapping the output of a linear equation to a value between 0 and 1 using the **sigmoid function**.\n",
        "\n",
        "\n",
        "\n",
        "**Note on Feature Scaling:**\n",
        "Typically, linear models require input features to be scaled (standardized) so that variables with large magnitudes (like **balance** or **duration**) do not disproportionately influence the model compared to binary variables (0/1). However, we **excluded an explicit StandardScaler step** from our pipeline. This is because PySpark's **LogisticRegression** implementation includes a built-in parameter **standardization=True** (enabled by default). This automatically handles the scaling of features during the training process to ensure numerical stability and correct regularization, simplifying our workflow."
      ],
      "metadata": {
        "id": "tZSmPhFSaKFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No feature selection"
      ],
      "metadata": {
        "id": "W_jb9Nhq-yfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before applying any filtering strategy, it is essential to train a model using **all available features** (the full set of 61 columns generated by the preprocessing). This serves as our **control group**.\n",
        "\n",
        "By training the Logistic Regression on the complete dataset, we establish a baseline performance level (Accuracy and AUC). This allows us to answer the critical question later: *Does removing features actually improve the model, or does it cause a loss of valuable information?*"
      ],
      "metadata": {
        "id": "853PNM9gbLJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# We create a Pipeline with Logistic Regression as its only step\n",
        "lr = LogisticRegression()\n",
        "pipeline_no_fs = Pipeline(stages=[lr])\n",
        "\n",
        "# We train the model\n",
        "model_no_fs = pipeline_no_fs.fit(train)\n",
        "\n",
        "# And finally make predictions on the validation set.\n",
        "predictions_no_fs = model_no_fs.transform(validation)\n",
        "predictions_no_fs.select('label', 'prediction', 'probability').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpxyu35s-3Vl",
        "outputId": "d13c8903-2659-46e4-c0c6-d024cc95c5dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+\n",
            "|label|prediction|         probability|\n",
            "+-----+----------+--------------------+\n",
            "|  1.0|       1.0|[1.66110065873039...|\n",
            "|  1.0|       1.0|[0.19288408764953...|\n",
            "|  0.0|       1.0|[0.17038390343469...|\n",
            "|  1.0|       0.0|[0.52568405879518...|\n",
            "|  1.0|       1.0|[0.03716517858075...|\n",
            "+-----+----------+--------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Selection using the FPR Strategy**"
      ],
      "metadata": {
        "id": "HE7TvBCyIccx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection was applied using the **False Positive Rate (FPR)** strategy. This approach is considered the least conservative, selecting a larger set of features that may be relevant for the model.  \n",
        "\n",
        "A significance threshold of 0.05 was used, retaining only features that show a statistically significant relationship with the target variable.  \n",
        "The objective is to evaluate whether this feature selection improves, maintains, or reduces the performance of the logistic regression model compared to the baseline model without feature selection.\n"
      ],
      "metadata": {
        "id": "jDVq-A8_j3rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import UnivariateFeatureSelector\n",
        "\n",
        "# Configure the feature selector using the FPR strategy\n",
        "selector_fpr = UnivariateFeatureSelector(\n",
        "    featuresCol=\"features\",\n",
        "    outputCol=\"selectedFeatures_fpr\",\n",
        "    labelCol=\"label\",\n",
        "    selectionMode=\"fpr\",\n",
        ")\n",
        "\n",
        "# Set the significance threshold to 0.05\n",
        "selector_fpr.setSelectionThreshold(0.05)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD0NDNznI4Zd",
        "outputId": "fa5b5f14-9265-4af3-c8d8-ee721cf70771"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UnivariateFeatureSelector_2a93f70cffb5"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, with the preprocessing already made all input variables are considered continuous, and the target variable is treated as categorical. This setup ensures that the Univariate Feature Selector can correctly evaluate the statistical relevance of each feature with respect to the target, allowing the selection process to proceed under the appropriate assumptions for the data types.\n"
      ],
      "metadata": {
        "id": "ZC7MnDWxkxhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify that all input features are treated as continuous\n",
        "selector_fpr.setFeatureType(\"continuous\")\n",
        "\n",
        "# Specify that the target variable is categorical\n",
        "selector_fpr.setLabelType(\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNUChcHwJY1z",
        "outputId": "166befb1-a81f-4f0c-8104-31510c5e1d92"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UnivariateFeatureSelector_2a93f70cffb5"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After configuring the feature selection using the FPR strategy, a logistic regression model is applied to the selected features.  \n",
        "\n",
        "The pipeline is constructed to combine the feature selector and the classifier, ensuring that the selection process and model training are performed sequentially.  \n",
        "\n",
        "The model is then trained on the training dataset, and predictions are generated for the validation set in order to assess its performance.\n"
      ],
      "metadata": {
        "id": "729FlVh6lZ0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the Logistic Regression model to use the features selected by the FPR strategy\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"selectedFeatures_fpr\",\n",
        "    labelCol=\"label\"\n",
        ")\n",
        "\n",
        "# Create a pipeline including the feature selector and the logistic regression model\n",
        "pipeline_fpr = Pipeline(stages=[selector_fpr, lr])\n",
        "\n",
        "# Train the model using the training data and generate predictions on the validation set\n",
        "model_fpr = pipeline_fpr.fit(train)\n",
        "predictions_fpr = model_fpr.transform(validation)\n"
      ],
      "metadata": {
        "id": "wjjbiYA-KQF5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model has been trained, the fitted feature selector is extracted from the pipeline in order to examine which features were retained by the FPR strategy.  \n",
        "\n",
        "The indices of the selected features are retrieved and compared to the total number of original features, providing an overview of the dimensionality reduction achieved.  \n",
        "\n",
        "This step allows assessing how many and which features the feature selection process considered most relevant for the model.\n"
      ],
      "metadata": {
        "id": "uqof0FlonZOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the fitted feature selector from the trained pipeline\n",
        "selector_model = model_fpr.stages[0]\n",
        "\n",
        "# Retrieve the indices of the features that were selected\n",
        "selected_indices = selector_model.selectedFeatures\n",
        "\n",
        "# Display the number of original features and the number of features selected by the FPR strategy\n",
        "print(f\"Original number of features: 61\")\n",
        "print(f\"Number of selected features: {len(selected_indices)}\")\n",
        "print(f\"Selected feature indices: {selected_indices}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxdtM1O_Kfct",
        "outputId": "9804fffe-3f07-40f9-f652-fe519c929bd5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 61\n",
            "Number of selected features: 43\n",
            "Selected feature indices: [13, 19, 4, 16, 55, 28, 29, 54, 14, 50, 35, 51, 37, 57, 36, 45, 1, 17, 18, 12, 9, 38, 49, 56, 47, 53, 22, 25, 46, 59, 32, 0, 42, 23, 40, 7, 8, 58, 44, 60, 31, 26, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Selection using the FWE Strategy**"
      ],
      "metadata": {
        "id": "tbtghoClN1WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature selection process is now applied using the **Family-Wise Error (FWE)** strategy.  \n",
        "\n",
        "This method is more conservative than the FPR approach, retaining only\n",
        "features that show a strong statistical relationship with the target variable.\n",
        "\n",
        "The objective is to evaluate how a stricter selection criterion affects the performance of the logistic regression model compared to both the baseline and the FPR-based selection.\n"
      ],
      "metadata": {
        "id": "lzBU5EO_oFbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the feature selector using the FWE strategy\n",
        "selector_fwe = UnivariateFeatureSelector(\n",
        "    featuresCol=\"features\",\n",
        "    outputCol=\"selectedFeatures\",\n",
        "    labelCol=\"label\",\n",
        "    selectionMode=\"fwe\"\n",
        ")\n",
        "\n",
        "# Set the significance threshold to 0.05\n",
        "selector_fwe.setSelectionThreshold(0.05)"
      ],
      "metadata": {
        "id": "gbJ8ZnnxN28E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64b6649-2ed1-4ad5-b7e9-76f7a9e65ee6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UnivariateFeatureSelector_fdcc738ccee1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Again, the input features are treated as continuous and the target variable as categorical, ensuring that the FWE-based feature selection evaluates the statistical relevance of each feature correctly.\n",
        "\n",
        "This configuration prepares the selector to apply the stricter significance criterion defined by the Family-Wise Error approach.\n"
      ],
      "metadata": {
        "id": "mPDX_bx7olQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selector_fwe.setFeatureType(\"continuous\")\n",
        "selector_fwe.setLabelType(\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4avhciiN3ks",
        "outputId": "6f927f39-daa3-4ccd-dfbe-d046946992ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UnivariateFeatureSelector_fdcc738ccee1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression model is applied to the features selected by the FWE strategy.  \n",
        "\n",
        "The pipeline is constructed and trained in the same way as for the FPR approach, allowing a direct comparison of model performance under the stricter feature selection criterion.\n"
      ],
      "metadata": {
        "id": "TEQG50sVpI9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_fwe = LogisticRegression(featuresCol='selectedFeatures', labelCol='label')\n",
        "\n",
        "# Create the pipeline specific to FWE selection\n",
        "pipeline_fwe = Pipeline(stages=[selector_fwe, lr_fwe])\n",
        "\n",
        "# 4. Train the model\n",
        "model_fwe = pipeline_fwe.fit(train)\n",
        "\n",
        "# 5. Generate predictions on the validation set\n",
        "predictions_fwe = model_fwe.transform(validation)\n"
      ],
      "metadata": {
        "id": "LiFAy-a9ODBG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the same way as before, after training the model, the fitted feature selector is extracted from the pipeline to examine which features were retained by the FWE strategy.\n",
        "\n",
        "The indices of the selected features are retrieved and compared to the total number of original features, providing an overview of the dimensionality reduction achieved.  \n",
        "\n",
        "This step allows assessing how many and which features the stricter FWE-based selection process considered most relevant for the model.\n"
      ],
      "metadata": {
        "id": "nAJxiYtLpZLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Retrieve the number of features retained by the FWE strategy\n",
        "selector_model_fwe = model_fwe.stages[0]\n",
        "\n",
        "# Extract the indices of the selected features\n",
        "selected_indices_fwe = selector_model_fwe.selectedFeatures\n",
        "\n",
        "# Display summary of feature selection\n",
        "print(f\"Original number of features: 61\")\n",
        "print(f\"Number of features selected (FWE): {len(selected_indices_fwe)}\")\n",
        "print(f\"Selected feature indices: {selected_indices_fwe}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWcLhF7QOJM1",
        "outputId": "c05a9bd5-d3f9-4cb8-cce8-07e62e415a54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 61\n",
            "Number of features selected (FWE): 37\n",
            "Selected feature indices: [13, 19, 4, 16, 55, 28, 29, 54, 14, 50, 35, 51, 37, 57, 45, 1, 17, 18, 12, 9, 38, 49, 56, 47, 25, 46, 59, 32, 42, 40, 7, 8, 58, 60, 31, 26, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Selection using Percentile Strategy**"
      ],
      "metadata": {
        "id": "Ngj603eyPJLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Percentile-based feature selection strategy selects a fixed proportion of the most relevant features, in this case the top 25% based on statistical relevance.  \n",
        "\n",
        "All input features are treated as continuous and the target as categorical, as in the previous approaches.  \n",
        "\n",
        "The main difference compared to the FPR and FWE strategies is that the selection threshold is defined as a proportion of features rather than a significance level, resulting in a consistent reduction to approximately 15–16 features from the original 61.\n"
      ],
      "metadata": {
        "id": "6MBtdJlLt7RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the feature selector using the Percentile strategy\n",
        "selector_pct = UnivariateFeatureSelector(\n",
        "    featuresCol=\"features\",\n",
        "    outputCol=\"selectedFeatures\",\n",
        "    labelCol=\"label\",\n",
        "    selectionMode=\"percentile\"\n",
        ")\n",
        "\n",
        "# Select the top 25% of features based on their statistical relevance\n",
        "selector_pct.setSelectionThreshold(0.25)\n",
        "\n",
        "selector_pct.setFeatureType(\"continuous\")\n",
        "selector_pct.setLabelType(\"categorical\")\n",
        "\n",
        "lr_pct = LogisticRegression(featuresCol='selectedFeatures', labelCol='label')\n",
        "pipeline_pct = Pipeline(stages=[selector_pct, lr_pct])\n",
        "\n",
        "model_pct = pipeline_pct.fit(train)\n",
        "\n",
        "predictions_pct = model_pct.transform(validation)\n",
        "\n",
        "selector_model_pct = model_pct.stages[0]\n",
        "selected_indices_pct = selector_model_pct.selectedFeatures\n",
        "\n",
        "print(f\"Original number of features: 61\")\n",
        "print(f\"Number of features selected (top 25%): {len(selected_indices_pct)}\")\n",
        "print(f\"Selected feature indices: {selected_indices_pct}\")"
      ],
      "metadata": {
        "id": "KNz3sgUUPLYP",
        "outputId": "c8426317-eedc-46ac-917f-8a1c28390630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 61\n",
            "Number of features selected (top 25%): 15\n",
            "Selected feature indices: [1, 5, 7, 12, 13, 14, 16, 18, 25, 26, 28, 29, 31, 32, 35]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Performance: Accuracy and AUC**"
      ],
      "metadata": {
        "id": "Nh8Id6VJuMRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of all four logistic regression models was evaluated using two complementary metrics: **accuracy** and **Area Under the Curve (AUC)**.  \n",
        "\n",
        "As known, accuracy measures the proportion of correct predictions over the total number of instances. However, accuracy depends on a fixed decision threshold (typically 0.5 in logistic regression), and therefore only reflects model performance at that specific cutoff.  \n",
        "\n",
        "The AUC, derived from the Receiver Operating Characteristic (ROC) curve, evaluates the model's discriminative ability across all possible thresholds. It represents the probability that a randomly chosen positive instance receives a higher predicted score than a randomly chosen negative instance. In this way, AUC captures the overall ranking quality of the model's predictions and is less sensitive to the choice of threshold. Even in the case of balanced classes, AUC is valuable for comparing models because it highlights differences in their ability to distinguish positive from negative cases that might not be apparent from accuracy alone.  \n",
        "\n",
        "Together, these metrics provide a more complete assessment of model performance, allowing for an informed comparison of the baseline logistic regression without feature selection and the three feature selection strategies (FPR, FWE, and top 25% percentile).\n"
      ],
      "metadata": {
        "id": "dTJhGOutuM_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "import pandas as pd\n",
        "\n",
        "# Accuracy evaluator\n",
        "evaluator_acc = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Calculate accuracy for each model\n",
        "acc_no_fs = evaluator_acc.evaluate(predictions_no_fs)\n",
        "acc_fpr = evaluator_acc.evaluate(predictions_fpr)\n",
        "acc_fwe = evaluator_acc.evaluate(predictions_fwe)\n",
        "acc_pct = evaluator_acc.evaluate(predictions_pct)\n",
        "\n",
        "# AUC evaluator\n",
        "evaluator_auc = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "# Calculate AUC for each model\n",
        "auc_no_fs = evaluator_auc.evaluate(predictions_no_fs)\n",
        "auc_fpr = evaluator_auc.evaluate(predictions_fpr)\n",
        "auc_fwe = evaluator_auc.evaluate(predictions_fwe)\n",
        "auc_pct = evaluator_auc.evaluate(predictions_pct)\n",
        "\n",
        "# Create a summary table\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": [\"No Feature Selection\", \"FPR\", \"FWE\", \"Top 25% Percentile\"],\n",
        "    \"Accuracy\": [acc_no_fs, acc_fpr, acc_fwe, acc_pct],\n",
        "    \"AUC\": [auc_no_fs, auc_fpr, auc_fwe, auc_pct]\n",
        "})\n",
        "\n",
        "# Display results\n",
        "results_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "qol1vMqFTDBm",
        "outputId": "0a9e6c44-c6ee-4685-954a-4b10cbbe8de7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  Model  Accuracy       AUC\n",
              "0  No Feature Selection  0.823890  0.906912\n",
              "1                   FPR  0.823418  0.906932\n",
              "2                   FWE  0.826251  0.907998\n",
              "3    Top 25% Percentile  0.653919  0.715262"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a29c8fdc-02d6-44ee-a5dc-90283921b952\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No Feature Selection</td>\n",
              "      <td>0.823890</td>\n",
              "      <td>0.906912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FPR</td>\n",
              "      <td>0.823418</td>\n",
              "      <td>0.906932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FWE</td>\n",
              "      <td>0.826251</td>\n",
              "      <td>0.907998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Top 25% Percentile</td>\n",
              "      <td>0.653919</td>\n",
              "      <td>0.715262</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a29c8fdc-02d6-44ee-a5dc-90283921b952')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a29c8fdc-02d6-44ee-a5dc-90283921b952 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a29c8fdc-02d6-44ee-a5dc-90283921b952');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9042612f-79bf-4149-9f05-11c00aaf3d2e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9042612f-79bf-4149-9f05-11c00aaf3d2e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9042612f-79bf-4149-9f05-11c00aaf3d2e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_51950e8f-7bdb-4169-9863-ff9fab62fd0e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_51950e8f-7bdb-4169-9863-ff9fab62fd0e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"FPR\",\n          \"Top 25% Percentile\",\n          \"No Feature Selection\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0853095990800187,\n        \"min\": 0.653918791312559,\n        \"max\": 0.826251180358829,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.8234183191690274,\n          0.653918791312559,\n          0.823890462700661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09601039749103087,\n        \"min\": 0.7152622848941965,\n        \"max\": 0.907997532067471,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9069316713536529,\n          0.7152622848941965,\n          0.9069119994277262\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table summarizes the performance of the logistic regression models under different feature selection strategies.  \n",
        "\n",
        "The baseline model without feature selection achieves an accuracy of 0.824 and an AUC of 0.907, providing a reference for comparison. The FPR-based selection results in virtually identical performance, indicating that the less conservative selection retains most relevant features without degrading model quality.  \n",
        "\n",
        "The FWE strategy slightly improves both metrics, with an accuracy of 0.826 and an AUC of 0.908, suggesting that a more conservative selection can slightly enhance the discriminative power of the model by removing marginally relevant features.  \n",
        "\n",
        "In contrast, the top 25% percentile strategy significantly reduces performance, with accuracy dropping to 0.654 and AUC to 0.715. This indicates that selecting only a small subset of features may discard important information, impairing both the overall correctness of predictions and the model's ability to rank positive instances above negatives.  \n",
        "\n",
        "Overall, the results highlight that moderate or conservative feature selection (FPR or FWE) can maintain or slightly improve model performance, while overly aggressive reduction (top 25%) can be detrimental.\n"
      ],
      "metadata": {
        "id": "DTt742ibvs7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Extracting and Interpreting Selected Feature Names**"
      ],
      "metadata": {
        "id": "GtQk6ckDTivK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section extracts the names of the features selected by the FWE-based feature selection strategy (strategy with best accuracy and best AUC).\n",
        "\n",
        "Using the metadata stored in the vectorized feature column, the function retrieves the original feature names corresponding to the indices selected by the model. This allows identifying which features the selection process deemed most relevant for the logistic regression model.\n"
      ],
      "metadata": {
        "id": "cIcLQhDEzaUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get original feature names from a VectorAssembler column\n",
        "def get_feature_names(df, features_col=\"features\"):\n",
        "    meta = df.schema[features_col].metadata\n",
        "    attrs = meta.get(\"ml_attr\", {}).get(\"attrs\", {})\n",
        "\n",
        "    all_attrs = []\n",
        "    for t in [\"numeric\", \"binary\", \"nominal\"]:\n",
        "        all_attrs += attrs.get(t, [])\n",
        "\n",
        "    all_attrs.sort(key=lambda x: x[\"idx\"])\n",
        "    return [x[\"name\"] for x in all_attrs]\n",
        "\n",
        "# Retrieve all feature names\n",
        "all_feature_names = get_feature_names(train)\n",
        "\n",
        "# Retrieve indices selected by the FWE model\n",
        "selected_indices = model_fwe.stages[0].selectedFeatures\n",
        "\n",
        "# Display selected features with their positions\n",
        "print(f\"Selected Features by FWE ({len(selected_indices)}): \")\n",
        "for idx in selected_indices:\n",
        "    name = all_feature_names[idx]\n",
        "    print(f\"Position {idx}: {name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnG0D7W5Tcpg",
        "outputId": "d6f92f8d-dcb8-43ab-91e3-e645ecb4cd8d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features by FWE (37): \n",
            "Position 13: marital_vec_married\n",
            "Position 19: education_vec_primary\n",
            "Position 4: job_vec_services\n",
            "Position 16: marital_vec___unknown\n",
            "Position 55: duration\n",
            "Position 28: loan_vec_no\n",
            "Position 29: loan_vec_yes\n",
            "Position 54: balance\n",
            "Position 14: marital_vec_single\n",
            "Position 50: month_vec_mar\n",
            "Position 35: poutcome_vec_unknown\n",
            "Position 51: month_vec_dec\n",
            "Position 37: poutcome_vec_success\n",
            "Position 57: contacted_before\n",
            "Position 45: month_vec_apr\n",
            "Position 1: job_vec_blue-collar\n",
            "Position 17: education_vec_secondary\n",
            "Position 18: education_vec_tertiary\n",
            "Position 12: job_vec___unknown\n",
            "Position 9: job_vec_entrepreneur\n",
            "Position 38: poutcome_vec_other\n",
            "Position 49: month_vec_sep\n",
            "Position 56: campaign\n",
            "Position 47: month_vec_oct\n",
            "Position 25: housing_vec_no\n",
            "Position 46: month_vec_feb\n",
            "Position 59: previous\n",
            "Position 32: contact_vec_unknown\n",
            "Position 42: month_vec_jul\n",
            "Position 40: month_vec_may\n",
            "Position 7: job_vec_student\n",
            "Position 8: job_vec_unemployed\n",
            "Position 58: days_since_last_contact\n",
            "Position 60: day\n",
            "Position 31: contact_vec_cellular\n",
            "Position 26: housing_vec_yes\n",
            "Position 5: job_vec_retired\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FWE (Family-Wise Error) strategy retained 37 features, reducing noise while keeping the variables most relevant to customer behavior. These features can be grouped into five main dimensions:\n",
        "\n",
        "1. Immediate Engagement  \n",
        "   - Feature: `duration`  \n",
        "   - Interpretation: Call duration emerges as a key predictor. Longer calls generally indicate that meaningful conversations took place and the client was engaged, whereas very short calls suggest low interest or immediate disengagement.\n",
        "\n",
        "2. Previous Campaign History  \n",
        "   - Features: `poutcome_success`, `poutcome_unknown`, `previous`, `contacted_before`, `days_since_last_contact`  \n",
        "   - Interpretation: Past interactions strongly influence future behavior. Clients who accepted previous offers are more likely to subscribe again. Additionally, the recency and frequency of contact help differentiate between clients who are currently engaged and those who are \"cold\" leads.\n",
        "\n",
        "3. Financial Stability & Liquidity  \n",
        "   - Features: `balance`, `housing_yes/no`, `loan_yes/no`  \n",
        "   - Interpretation: The model takes into account both the client’s available funds and existing debts. `balance` reflects savings, while `housing` and `loan` indicators capture liabilities, which can affect disposable income and the likelihood of investing.\n",
        "\n",
        "4. Seasonality  \n",
        "   - Features: Specific months (`mar`, `dec`, `sep`, `oct`, `apr`, `feb`, `may`, `jul`) and `day`  \n",
        "   - Interpretation: Selected months indicate seasonal patterns in subscriptions, likely tied to economic cycles, fiscal quarters, or periods of higher liquidity, such as bonuses or year-end savings.\n",
        "\n",
        "5. Demographic Profiles  \n",
        "   - Features: `job_retired`, `job_student`, `job_blue-collar`, `marital_single/married`, `education_primary/tertiary`  \n",
        "   - Interpretation: Certain socioeconomic segments appear more likely to subscribe. `job_retired` may indicate clients seeking low-risk investments, `job_student` first-time account holders, and marital or education status reflects different financial priorities and life stages.\n"
      ],
      "metadata": {
        "id": "r7Fn4w_-UknG"
      }
    }
  ]
}